# -*- coding: utf-8 -*-
"""Copy of Churn Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lltayIoTJIF_yULVy4B9lJeRUEK48X68
"""

# Install the necessary libraries (XGBoost is often pre-installed, but this ensures it)
!pip install pandas numpy scikit-learn xgboost matplotlib seaborn shap lime imblearn

# Import core libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report, roc_auc_score
import xgboost as xgb
import shap
import lime
import lime.lime_tabular

# Suppress warnings for cleaner notebook output
import warnings
warnings.filterwarnings('ignore')

from google.colab import files
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_csv(file_name)

print(df.head())
print(df.info())

# Convert 'TotalCharges' column to numeric, coercing errors to NaN
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Fill the few resulting NaN values with 0 (assuming very low tenure customers)
df['TotalCharges'] = df['TotalCharges'].fillna(0)

# Convert the target variable 'Churn' to 0 and 1
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Handle inconsistencies and drop unnecessary IDs
for col in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']:
    df[col] = df[col].replace('No internet service', 'No')

df = df.drop('customerID', axis=1)

# Apply One-Hot Encoding to all remaining categorical columns
df_encoded = pd.get_dummies(df, drop_first=True)

print(f"\nShape after encoding: {df_encoded.shape}")

# Define Features (X) and Target (y)
X = df_encoded.drop('Churn', axis=1)
y = df_encoded['Churn']

# Split data (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply SMOTE to balance the training data only
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"\nOriginal Churn count in Train: {y_train.sum()}")
print(f"Resampled Churn count in Train: {y_train_resampled.sum()}")

# Initialize and train the XGBoost classifier
# eval_metric='logloss' is standard for binary classification
xgb_model = xgb.XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

print("Training XGBoost model...")
xgb_model.fit(X_train_resampled, y_train_resampled)
print("Training complete.")

# Make predictions (probabilities are needed for AUC)
y_pred = xgb_model.predict(X_test)
y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]

# Detailed evaluation report
print("\n--- Classification Report ---")
print(classification_report(y_test, y_pred))

# Calculate AUC Score (your key metric for imbalanced data)
auc = roc_auc_score(y_test, y_pred_proba)
print(f"Area Under the Curve (AUC): {auc:.4f}")

# Store this AUC score for your resume!

# Create the SHAP Explainer
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)

print("\n--- Global Feature Importance (SHAP) ---")
# SHAP Summary Plot in a Colab-compatible way (using the second element for classification)
# This plot shows the top features overall and their direction of impact
shap.summary_plot(shap_values, X_test)

# Prepare the LIME Explainer
# We use the original training data values/columns for the explainer's context
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['No Churn', 'Churn'], # Names corresponding to 0 and 1
    mode='classification',
    random_state=42
)

# Select a customer that the model predicted as high-risk (e.g., customer at index 5 in the test set)
customer_index = 5
instance_to_explain = X_test.iloc[customer_index].values

# Generate the local explanation
explanation = lime_explainer.explain_instance(
    data_row=instance_to_explain,
    predict_fn=xgb_model.predict_proba,
    num_features=5 # Focus on the 5 most important features for THIS customer
)

print(f"\n--- Local Explanation for Customer {customer_index} ---")
# Display the explanation HTML directly in the Colab notebook
explanation.show_in_notebook(show_table=True)

# 1. Create a results DataFrame from the X_test index
results_df = X_test.copy()

# 2. Add prediction and probability columns
results_df['Actual_Churn'] = y_test
results_df['Churn_Probability'] = y_pred_proba
results_df['Predicted_Churn'] = y_pred

# 3. Create the Risk Segment Column
def risk_segment(prob):
    if prob >= 0.7:
        return 'High Risk'
    elif prob >= 0.4:
        return 'Medium Risk'
    else:
        return 'Low Risk'

results_df['Risk_Segment'] = results_df['Churn_Probability'].apply(risk_segment)

# 4. Save the final data to a CSV file
results_df.to_csv('Churn_Prediction_Results.csv', index=False)